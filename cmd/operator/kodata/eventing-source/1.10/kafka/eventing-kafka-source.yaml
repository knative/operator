# Copyright 2021 The Knative Authors
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
apiVersion: v1
kind: ConfigMap
metadata:
  name: config-kafka-source-data-plane
  namespace: knative-eventing
  labels:
    app.kubernetes.io/version: "f47a52f70f7bc4f06549f494be6a77f93a5886b1"
  annotations:
    knative.dev/example-checksum: "8157ecb1"
data:
  _example: |
    ################################
    #                              #
    #    EXAMPLE CONFIGURATION     #
    #                              #
    ################################

    # This block is not actually functional configuration,
    # but serves to illustrate the available configuration
    # options and document them in a way that is accessible
    # to users that `kubectl edit` this config map.
    #
    # These sample configuration options may be copied out of
    # this example block and unindented to be in the data block
    # to actually change the configuration.

    # All configurations in this ConfigMap are globally applied to each
    # resource and there is no way to change them on a per-resource basis,
    # unless otherwise specified.

    # Consumer configuration are documented in https://kafka.apache.org/documentation/#consumerconfigs.
    # Some configurations might be forced by the actual code to make sure we respect the Knative Eventing
    # delivery constraints, for example, `key.deserializer` and `value.deserializer`.
    config-kafka-source-consumer.properties: |
      key.deserializer=org.apache.kafka.common.serialization.StringDeserializer
      value.deserializer=io.cloudevents.kafka.CloudEventDeserializer
      fetch.min.bytes=1

    # Available Vertx WebClientOptions are documented in
    # https://vertx.io/docs/apidocs/io/vertx/ext/web/client/WebClientOptions.html.
    #
    # Each egress resource (KafkaSource, Trigger, Subscription) creates an HTTP client in each pod where the resource is
    # scheduled, meaning that a client isn't shared across multiple resources to provide better isolation.
    #
    # The mapping is the following:
    #  for each method starting with `set` there is a property that can be set with the name that follows the `set`
    #  prefix starting with a lowercase letter.
    # For example, there is a method called `setIdleTimeout` and the associated property is `idleTimeout`.
    config-kafka-source-webclient.properties: |
      idleTimeout=10000
  config-kafka-source-producer.properties: |
    key.serializer=org.apache.kafka.common.serialization.StringSerializer
    value.serializer=io.cloudevents.kafka.CloudEventSerializer
    acks=all
    buffer.memory=33554432
    # compression.type=snappy
    retries=2147483647
    batch.size=16384
    client.dns.lookup=use_all_dns_ips
    connections.max.idle.ms=600000
    delivery.timeout.ms=120000
    linger.ms=0
    max.block.ms=60000
    max.request.size=1048576
    partitioner.class=org.apache.kafka.clients.producer.internals.DefaultPartitioner
    receive.buffer.bytes=-1
    request.timeout.ms=2000
    enable.idempotence=false
    max.in.flight.requests.per.connection=5
    metadata.max.age.ms=300000
    # metric.reporters=""
    metrics.num.samples=2
    metrics.recording.level=INFO
    metrics.sample.window.ms=30000
    reconnect.backoff.max.ms=1000
    reconnect.backoff.ms=50
    retry.backoff.ms=100
    # transaction.timeout.ms=60000
    # transactional.id=null
  config-kafka-source-consumer.properties: |
    cloudevent.invalid.transformer.enabled=true
    cloudevent.invalid.kind.plural=kafkasources
    key.deserializer=org.apache.kafka.common.serialization.StringDeserializer
    value.deserializer=io.cloudevents.kafka.CloudEventDeserializer
    fetch.min.bytes=1
    heartbeat.interval.ms=3000
    max.partition.fetch.bytes=65536
    session.timeout.ms=10000
    # ssl.key.password=
    # ssl.keystore.location=
    # ssl.keystore.password=
    # ssl.truststore.location=
    # ssl.truststore.password=
    allow.auto.create.topics=true
    auto.offset.reset=earliest
    client.dns.lookup=use_all_dns_ips
    connections.max.idle.ms=540000
    default.api.timeout.ms=2000
    enable.auto.commit=false
    exclude.internal.topics=true
    fetch.max.bytes=52428800
    isolation.level=read_uncommitted
    max.poll.interval.ms=300000
    max.poll.records=50
    partition.assignment.strategy=org.apache.kafka.clients.consumer.StickyAssignor
    receive.buffer.bytes=65536
    request.timeout.ms=2000
    # sasl.client.callback.handler.class=
    # sasl.jaas.config=
    # sasl.kerberos.service.name=
    # sasl.login.callback.handler.class
    # sasl.login.class
    # sasl.mechanism
    security.protocol=PLAINTEXT
    send.buffer.bytes=131072
    # ssl.enabled.protocols=
    # ssl.keystore.type=
    # ssl.protocol=
    # ssl.provider=
    auto.commit.interval.ms=5000
    check.crcs=true
    # client.rack=
    fetch.max.wait.ms=500
    # interceptor.classes=
    metadata.max.age.ms=600000
    # metrics.reporters=
    # metrics.num.samples=
    # metrics.recording.level=INFO
    # metrics.sample.window.ms=
    reconnect.backoff.max.ms=1000
    retry.backoff.ms=100
    # sasl.kerberos.kinit.cmd=
    # sasl.kerberos.min.time.before.relogin=
    # sasl.kerberos.ticket.renew.jitter=
    # sasl.login.refresh.buffer.seconds=
    # sasl.login.refresh.min.period.seconds=
    # sasl.login.refresh.window.factor
    # sasl.login.refresh.window.jitter
    # security.providers
    # ssl.cipher.suites
    # ssl.endpoint.identification.algorithm
    # ssl.keymanager.algorithm
    # ssl.secure.random.implementation
    # ssl.trustmanager.algorithm
  config-kafka-source-webclient.properties: |
    idleTimeout=10000
    maxPoolSize=100

---
# Copyright 2021 The Knative Authors
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: knative-kafka-source-data-plane
  labels:
    app.kubernetes.io/version: "f47a52f70f7bc4f06549f494be6a77f93a5886b1"
rules:
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - list
      - watch

---
# Copyright 2021 The Knative Authors
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
apiVersion: v1
kind: ServiceAccount
metadata:
  name: knative-kafka-source-data-plane
  namespace: knative-eventing
  labels:
    app.kubernetes.io/version: "f47a52f70f7bc4f06549f494be6a77f93a5886b1"

---
# Copyright 2021 The Knative Authors
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: knative-kafka-source-data-plane
  labels:
    app.kubernetes.io/version: "f47a52f70f7bc4f06549f494be6a77f93a5886b1"
subjects:
  - kind: ServiceAccount
    name: knative-kafka-source-data-plane
    namespace: knative-eventing
roleRef:
  kind: ClusterRole
  name: knative-kafka-source-data-plane
  apiGroup: rbac.authorization.k8s.io

---
# Copyright 2021 The Knative Authors
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: kafka-source-dispatcher
  namespace: knative-eventing
  labels:
    app: kafka-source-dispatcher
    app.kubernetes.io/version: "f47a52f70f7bc4f06549f494be6a77f93a5886b1"
    app.kubernetes.io/component: kafka-source-dispatcher
    app.kubernetes.io/name: knative-eventing
spec:
  serviceName: kafka-source-dispatcher
  podManagementPolicy: "Parallel"
  selector:
    matchLabels:
      app: kafka-source-dispatcher
  template:
    metadata:
      name: kafka-source-dispatcher
      labels:
        app: kafka-source-dispatcher
        app.kubernetes.io/version: "f47a52f70f7bc4f06549f494be6a77f93a5886b1"
        app.kubernetes.io/component: kafka-channel-dispatcher
        app.kubernetes.io/name: knative-eventing
        app.kubernetes.io/kind: kafka-dispatcher
    spec:
      # To avoid node becoming SPOF, spread our replicas to different nodes and zones.
      topologySpreadConstraints:
        - maxSkew: 2
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app: kafka-source-dispatcher
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app: kafka-source-dispatcher
                topologyKey: kubernetes.io/hostname
              weight: 100
      serviceAccountName: knative-kafka-source-data-plane
      securityContext:
        runAsNonRoot: true
      containers:
        - name: kafka-source-dispatcher
          image: gcr.io/knative-releases/knative-kafka-broker-dispatcher:v1.10.1
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - mountPath: /etc/config
              name: config-kafka-source-data-plane
              readOnly: true
            - mountPath: /etc/contract-resources
              name: contract-resources
              readOnly: true
            - mountPath: /tmp
              name: cache
            - mountPath: /etc/logging
              name: kafka-config-logging
              readOnly: true
            - mountPath: /etc/tracing
              name: config-tracing
              readOnly: true
          ports:
            - containerPort: 9090
              name: http-metrics
              protocol: TCP
          env:
            - name: SERVICE_NAME
              value: "kafka-source-dispatcher"
            - name: SERVICE_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: PRODUCER_CONFIG_FILE_PATH
              value: /etc/config/config-kafka-source-producer.properties
            - name: CONSUMER_CONFIG_FILE_PATH
              value: /etc/config/config-kafka-source-consumer.properties
            - name: WEBCLIENT_CONFIG_FILE_PATH
              value: /etc/config/config-kafka-source-webclient.properties
            - name: DATA_PLANE_CONFIG_FILE_PATH
              value: /etc/contract-resources/data
            - name: EGRESSES_INITIAL_CAPACITY
              value: "20"
            - name: INSTANCE_ID
              valueFrom:
                fieldRef:
                  fieldPath: metadata.uid
            - name: METRICS_PATH
              value: /metrics
            - name: METRICS_PORT
              value: "9090"
            - name: METRICS_PUBLISH_QUANTILES
              value: "false"
            - name: METRICS_JVM_ENABLED
              value: "false"
            - name: CONFIG_TRACING_PATH
              value: "/etc/tracing"
            # https://github.com/fabric8io/kubernetes-client/issues/2212
            - name: HTTP2_DISABLE
              value: "true"
            # This should be set according to initial delay seconds
            - name: WAIT_STARTUP_SECONDS
              value: "8"
            - name: JAVA_TOOL_OPTIONS
              value: "-XX:+CrashOnOutOfMemoryError -XX:InitialRAMPercentage=70.0 -XX:MinRAMPercentage=70.0 -XX:MaxRAMPercentage=70.0"
          resources:
            requests:
              cpu: 1000m
              # 600Mi for virtual replicas + 100Mi overhead
              memory: 700Mi
            limits:
              cpu: 2000m
              memory: 1000Mi
          livenessProbe:
            failureThreshold: 3
            tcpSocket:
              port: 9090
            initialDelaySeconds: 10
            periodSeconds: 3
            successThreshold: 1
            timeoutSeconds: 1
          readinessProbe:
            failureThreshold: 3
            httpGet:
              port: 9090
              path: /metrics
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 3
            successThreshold: 1
            timeoutSeconds: 1
          terminationMessagePolicy: FallbackToLogsOnError
          terminationMessagePath: /dev/temination-log
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            capabilities:
              drop:
                - ALL
            seccompProfile:
              type: RuntimeDefault
      volumes:
        - name: config-kafka-source-data-plane
          configMap:
            name: config-kafka-source-data-plane
        - name: cache
          emptyDir: {}
        - name: kafka-config-logging
          configMap:
            name: kafka-config-logging
        - name: config-tracing
          configMap:
            name: config-tracing
      restartPolicy: Always
      dnsConfig:
        options:
          - name: single-request-reopen

---
